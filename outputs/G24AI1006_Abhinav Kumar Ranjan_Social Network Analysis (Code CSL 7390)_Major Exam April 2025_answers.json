[
    {
        "question_number": "1a",
        "subparts": {
            "question": "Consider the following incidence matrix of a simple undirected graph. Convert this into an adjacency matrix representation.",
            "answer": "The incidence matrix provided is: 1 0 1 1 1 0 1 0 0 0 1. Each edge in an incidence matrix corresponds to a row and every node has its designated column. From the incidence matrix: Edge 1 connects Node 1 and Node 2. Edge 2 connects Node 2 and Node 3. Edge 3 connects Node 2 and Node 4."
        }
    },
    {
        "question_number": "1b",
        "subparts": {
            "question": "Which network model assumes that edges are formed between pairs of nodes with a uniform probability, independent of other edges?",
            "answer": "Option - (B) Erdős-Rényi (Random Network) Model. In the Erdős-Rényi model, the edges connect randomly because every edge formation remains isolated from other edges."
        }
    },
    {
        "question_number": "1c",
        "subparts": {
            "question": "In game theory, a situation where no player can improve their outcome by unilaterally changing their strategy, given the strategies of other players, is known as:",
            "answer": "Option - (C) Nash Equilibrium. Player reach a stable position known as Nash Equilibrium when none of them achieve higher rewards through independent strategy modifications."
        }
    },
    {
        "question_number": "1d",
        "subparts": {
            "question": "The tendency for individuals in a social network to associate and bond with similar others is defined as:",
            "answer": "Option - (B) Assortative Mixing. Under the concept of Assortative Mixing, similar nodes establish connections with each other."
        }
    },
    {
        "question_number": "1e",
        "subparts": {
            "question": "Why might betweenness centrality be a more relevant measure than degree centrality for identifying critical nodes in a network transmitting information that must follow specific paths?",
            "answer": "Option (D) Because it quantifies how often a node lies on the shortest paths between other nodes. The measurement technique called Betweenness Centrality evaluates nodes based on their capacity to link other nodes because it determines information flow efficiencies."
        }
    },
    {
        "question_number": "1f",
        "subparts": {
            "question": "A key finding about scale-free networks (like those generated by the Barabási-Albert model) is their robustness to random node failures but vulnerability to targeted attacks on hubs. What underlying property best explains this?",
            "answer": "Option - (C) The presence of many nodes with very high degrees (hubs) that maintain connectivity. Scale-free networks operate through central nodes which serve as crucial connectors, so their attack vulnerability becomes particularly pronounced."
        }
    },
    {
        "question_number": "1g",
        "subparts": {
            "question": "In community detection, optimizing for high modularity aims to find partitions where:",
            "answer": "Option - (A) The number of intra-community edges is significantly higher than expected in a random network with the same degree sequence. The goal of community detection (modularity) is to identify communities through dense connections within those groups compared to random chance expectation."
        }
    },
    {
        "question_number": "1h",
        "subparts": {
            "question": "Consider two nodes, X and Y. The neighbors of X are A, B, C, D. The neighbors of Y are C, D, E. What is the Jaccard Coefficient for link prediction between X and Y?",
            "answer": "Option - (B) 2/5. Jaccard Coefficient is calculated as the intersection divided by the union of the neighbors."
        }
    },
    {
        "question_number": "1i",
        "subparts": {
            "question": "In the context of information cascade models, how does the activation mechanism differ fundamentally between the Independent Cascade Model (ICM) and the Linear Threshold Model (LTM)?",
            "answer": "Option - (A) ICM uses edge probabilities independently; LTM uses a weighted sum of active neighbors compared to a node threshold. ICM operates through independent edge probabilities but LTM adopts a threshold-based method which takes the weighted sum of neighbors' impact."
        }
    },
    {
        "question_number": "1j",
        "subparts": {
            "question": "A standard Graph Convolutional Network (GCN) aggregates information from a node's immediate neighbors. Why might this standard message-passing approach be suboptimal for tasks like node classification in networks with high heterophily (where connected nodes tend to be dissimilar)?",
            "answer": "Option-(B) Because aggregating features from dissimilar neighbors can blur the node's own representative features, making classification harder. When heterophily occurs during GCN operations, it becomes difficult to classify nodes because they lose their individual characteristics through the features aggregation process from dissimilar neighbors."
        }
    },
    {
        "question_number": "2",
        "subparts": {
            "question": "A novel influenza strain (following an SIR - Susceptible, Infected, Recovered model) is spreading in a city. You have access to a network graph representing close social contacts (nodes=people, edges=contacts). Resources are limited, allowing you to preemptively vaccinate (move directly to the 'Recovered' state) only 5% of the population. Describe a strategy using at least two distinct network analysis concepts that can be applied together to identify the individuals to vaccinate to most effectively minimize the total number of infections. Justify why your chosen concepts are appropriate and how they would be applied together.",
            "answer": "For reducing influenza transmission to 5% level, a combined approach of centrality measurement should be used to determine who gets vaccinated first. A high value of betweenness centrality reveals that the node functions as a 'bridge' through which the disease transmits to numerous other connections. The immunization of these specific people breaks transmission pathways between different segments of the network. High degree centrality helps identify 'hubs' which possess multiple connections since they have the potential to infect numerous other people. A computation between betweenness and degree centrality should be performed for every individual. A list of ranked people is formed based on the centrality measurement result. The group of people selected for vaccination needs a strong ranking in both centrality measures. Additional individuals should be included for vaccination until 5% coverage is reached if the top selected candidates do not suffice."
        }
    },
    {
        "question_number": "3",
        "subparts": {
            "question": "You are tasked with improving the ‘suggested collaborators' feature on a platform for academic researchers. Explain how you could combine link prediction algorithms with node embedding techniques (e.g., Node2Vec trained on paper citation/co-authorship networks) to generate recommendations. Discuss the role of homophily (researchers collaborating within similar fields) in this context and suggest one potential way to promote cross-disciplinary collaborations using your proposed system.",
            "answer": "The link prediction system utilizes link prediction algorithms that generate potential collaboration predictions through the analysis of current co-authorship and citation data. When researchers A and B demonstrate numerous joint citations, the link prediction method could indicate potential collaboration between them. Through Node2Vec, the system creates vector representations of researchers based on network structural information. Embeddings that align with each other between researchers demonstrate similar research interests and collaboration potential. Due to the natural human tendency of homophily, the system will provide recommendations related to researchers within similar academic fields. To encourage diverse collaborative ties, the system requires a diversity metric within its recommendation functionality. Although the link prediction score may be slightly lower, the system will give priority to research links that combine fields from distinct areas."
        }
    },
    {
        "question_number": "4a",
        "subparts": {
            "question": "Describe the core idea behind the Girvan-Newman algorithm for community detection.",
            "answer": "The Girvan-Newman algorithm detects communities through a processing method which successively eliminates edges connecting different communities."
        }
    },
    {
        "question_number": "4b",
        "subparts": {
            "question": "Explain how it uses edge betweenness centrality iteratively.",
            "answer": "Edge betweenness centrality analyzes the number of times an edge lies between all pairs of nodes to discover these edges. Edges with high betweenness values link separate communities according to this method."
        }
    },
    {
        "question_number": "4c",
        "subparts": {
            "question": "What is a major computational limitation of this algorithm?",
            "answer": "The cost of computation increases substantially when calculating edge betweenness centrality for extensive networks, especially when they involve frequent edge removal operations."
        }
    },
    {
        "question_number": "4d",
        "subparts": {
            "question": "Briefly explain how the Louvain method provides a more scalable alternative for optimizing modularity.",
            "answer": "The Louvain method functions as a scalable approach which uses greedy optimization to shift nodes between communities during its iterative process."
        }
    },
    {
        "question_number": "5a",
        "subparts": {
            "question": "Explain the intuition behind the PageRank algorithm for determining node importance.",
            "answer": "According to PageRank, the importance of nodes depends on both the quantity and quality of incoming links passing through them. Page importance derives from other pages that link to it."
        }
    },
    {
        "question_number": "5b",
        "subparts": {
            "question": "Describe the role of the 'damping factor' (d) for random surfer based PageRank algorithm.",
            "answer": "During random web navigation, a surfer has a probability expressed through the damping factor (d) to either pause their link click or move to an arbitrary page."
        }
    },
    {
        "question_number": "5c",
        "subparts": {
            "question": "What problem arises from ‘dangling nodes' (nodes with no outgoing links), and how is this typically handled in the PageRank calculation to ensure convergence?",
            "answer": "The lack of outgoing links from dangling nodes makes PageRank flow through the network. We handle this condition using an equal distribution among all network nodes. Each iteration distributes the rank of dangling nodes across all network nodes in an equal way."
        }
    },
    {
        "question_number": "6a",
        "subparts": {
            "question": "Identify all Pure Strategy Nash Equilibria in this game. Briefly explain why they are equilibria.",
            "answer": "There are two pure strategy Nash equilibria: (U, A) and (L, B). In (U, A), Player 1 cannot improve by switching to L, and Player 2 cannot improve by switching to B. In (L, B), Player 1 cannot improve by switching to U, and Player 2 cannot improve by switching to A."
        }
    },
    {
        "question_number": "6b",
        "subparts": {
            "question": "Suppose Player 1 chooses 'Strategy U' with probability p and ‘Strategy L' with probability 1-p. Calculate the expected payoff for Player 2 for each of the strategy.",
            "answer": "Expected payoff for Player 2 if choosing Strategy A: E[A] = 2p. Expected payoff for Player 2 if choosing Strategy B: E[B] = 3 - 2p."
        }
    },
    {
        "question_number": "6c",
        "subparts": {
            "question": "What will be the expected outcome if p = 0.7?",
            "answer": "E[A] = 2 * 0.7 = 1.4. E[B] = 3 - 2 * 0.7 = 1.6. Since E[B] > E[A], Player 2 could choose Strategy B. For Player 1, with (0.7U, 0.3L), the expected payoff is 0.6 when Player 2 chooses B."
        }
    },
    {
        "question_number": "7",
        "subparts": {
            "question": "Consider the following simple directed graph where edges point towards node B: A → B, C → B, D → B. We want to compute the updated feature vector for node B, denoted as h, using one layer of a simple Graph Neural Network.",
            "answer": "Step-1 Aggregate neighbor features: h(0) = 1/3 * ([1] + [3] + [3]) = [2]. Step-2 Transform: W * [2] = [0.5, 0.5]. Step-3 Activate: ReLU([0.5, 0.5]) = [0.5, 0.5]. Final Answer hB(1) = [0.5, 0.5]."
        }
    }
]